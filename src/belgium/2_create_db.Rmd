---
title: "Create database and filter data"
author:
- Damiano Oldoni
- Peter Desmet
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
---

In this document we transform the GBIF download - a csv file containing the occurrences of species for Belgium - into a sqlite database, to handle the large volume of data. Then we filter on issues and occurrence status. Note: some of these steps can take long.

# Setup

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

Load libraries:

```{r load_libraries}
library(tidyverse)      # To do datascience
library(tidylog)        # To provide feedback on dplyr functions
library(here)           # To find files
library(rgbif)          # To use GBIF services
library(glue)           # To write queries
library(RSQLite)        # To interact with SQlite databases
library(inborutils)     # To write a SQLite database from a csv file
```

# Transform CSV to sqlite file

## Get CSV file from GBIF

Download the occurrences from GBIF, based on the download key returned in `download.Rmd`:

```{r get_occ_file}
key <- "0125107-200613084148143"
zip_filename <- paste0(key, ".zip")
zip_path <- here::here("data", "raw", zip_filename)
if (!file.exists(zip_path)) {
  occ <- occ_download_get(
    key = key,
    path = here::here("data", "raw")
  )
}
```

Unzip the occurrence text file from the download file:

```{r unzip_csv}
occ_file <- paste(key, "occurrence.txt", sep = "_")
occ_path <- here::here("data", "raw", occ_file)

if (!file.exists(here::here("data", "raw", occ_file))) {
  unzip(zipfile = zip_path,
        files = "occurrence.txt",
        exdir = here::here("data", "raw"))
  file.rename(from = here::here("data", "raw", "occurrence.txt"),
              to = occ_path
  )
}
``` 

Columns names:

```{r get_cols}
cols_occ_file <- read_delim(occ_path, "\t", n_max = 1, quote = "")
cols_occ_file <- names(cols_occ_file)
cols_occ_file
```

Number of columns:

```{r n_cols}
length(cols_occ_file)
```

## Create sqlite file

### Database name, path and table name

Define name and path of `.sqlite` file:

```{r sqlite_path}
sqlite_file <- paste(key, "occurrence.sqlite", sep = "_")
sqlite_path <- here::here("data", "interim", sqlite_file)
```

And table name:

```{r define_table_name}
table_name <- "occ_all"
```

### Define columns to read

We define a subset of columns, `cols_to_use`, we are interested to, and their type:

```{r columns_to_use}
cols_to_use <- cols_only(
  "gbifID" = col_double(),
  "scientificName" = col_character(),
  "kingdom" = col_character(),
  "phylum" = col_character(),
  "class" = col_character(),
  "order" = col_character(),
  "family" = col_character(),
  "genus" = col_character(),
  "specificEpithet" = col_character(),
  "infraspecificEpithet" = col_character(),
  "taxonRank" = col_character(), 
  "taxonomicStatus" = col_character(),
  "datasetKey" = col_character(),
  "basisOfRecord" = col_character(),
  "occurrenceStatus" = col_character(),
  "lastInterpreted" = col_datetime(),
  "hasCoordinate" = col_logical(),
  "hasGeospatialIssues" = col_logical(),
  "decimalLatitude" = col_double(),
  "decimalLongitude" = col_double(),
  "coordinateUncertaintyInMeters" = col_double(),
  "coordinatePrecision" = col_double(),
  "pointRadiusSpatialFit" = col_double(),
  "verbatimCoordinateSystem" = col_character(),
  "verbatimSRS" = col_character(),
  "eventDate" = col_date(),
  "startDayOfYear" = col_double(),
  "endDayOfYear" = col_double(),
  "year" = col_double(),
  "month" = col_double(),
  "day" = col_double(),
  "verbatimEventDate" = col_character(),
  "samplingProtocol" = col_character(),
  "samplingEffort" = col_character(),
  "issue" = col_character(),
  "identificationVerificationStatus" = col_character(),
  "taxonKey" = col_double(),
  "acceptedTaxonKey" = col_double(),
  "kingdomKey" = col_double(),
  "phylumKey" = col_double(),
  "classKey" = col_double(),
  "orderKey" = col_double(),
  "familyKey" = col_double(),
  "genusKey" = col_double(),
  "subgenusKey" = col_double(),
  "speciesKey" = col_double(),
  "species" = col_character()
)
```

Columns not present in csv:

```{r cols_not_present}
names(cols_to_use$cols)[!names(cols_to_use$cols) %in% cols_occ_file]
```

### Write csv to sqlite

```{r csv_to_sqlite}
csv_to_sqlite(csv_file = occ_path,
              sqlite_file = sqlite_path,
              table_name = table_name,
              pre_process_size = 50000, 
              chunk_size = 10000,
              delim = "\t", 
              col_types = cols_to_use
)
```

## Overview
Open connection to database:

```{r open_connection_to_db}
sqlite_occ <- dbConnect(SQLite(), dbname = sqlite_path)
```

Number of columns present:

```{r check_fields_present}
cols_occ_db <- dbListFields(sqlite_occ, table_name)
length(cols_occ_db)
```

Check: selected columns not in the SQLite database:

```{r cols_not_present}
names(cols_to_use$cols)[!names(cols_to_use$cols) %in% cols_occ_db]
```

Check: columns in SQLite database not in selected columns:

```{r}
cols_occ_db[!cols_occ_db %in% names(cols_to_use$cols)]
```

Number of occurrences:

```{r n_occs_raw}
query <- glue_sql(
    "SELECT COUNT() FROM {table}",
    table = table_name,
    .con = sqlite_occ
  )
n_occs_total <- dbGetQuery(sqlite_occ, query)
n_occs_total <- n_occs_total$`COUNT()`
n_occs_total
```

Preview first 100 rows from table `occ_all`:

```{r preview_df}
query <- glue_sql("SELECT * FROM {big_table} LIMIT 100",
                  big_table = table_name,
                  .con = sqlite_occ
)
preview_df <- dbGetQuery(conn = sqlite_occ, query)
preview_df
```



# Filter data

## Define filters on occurrences

Occurrences containing the following issues should be filtered out:

```{r issues}
issues_to_discard <- c(
  "ZERO_COORDINATE",
  "COORDINATE_OUT_OF_RANGE", 
  "COORDINATE_INVALID",
  "COUNTRY_COORDINATE_MISMATCH"
)
```

Occurrences with the following occurrence status should be filtered out as well:

```{r occurrenceStatus}
occurrenceStatus_to_discard <- c(
  "absent",
  "excluded"
)
```

We won't take into account unverified observations neither:

```{r identificationVerificationStatus}
identificationVerificationStatus_to_discard <- c(
  "unverified",
  "unvalidated",
  "not able to validate",
  "control could not be conclusive due to insufficient knowledge",
  "unconfirmed",
  "unconfirmed - not reviewed",
  "validation requested"
)
```

We create an index based on these three columns if not already present:

```{r create_idx_occStatus_issue}
idx_occStatus_issue <- "idx_verifStatus_occStatus_issue"
# get indexes on table
query <- glue_sql(
    "PRAGMA index_list({table_name})",
    table_name = table_name,
    .con = sqlite_occ
)
indexes_all <- dbGetQuery(sqlite_occ, query)

# create index if not present
if (!idx_occStatus_issue %in% indexes_all$name) {
  query <- glue_sql(
  "CREATE INDEX {`idx`} ON {table_name} ({`cols_idx`*})",
  idx = idx_occStatus_issue,
  table_name = table_name,
  cols_idx = c("identificationVerificationStatus",
               "occurrenceStatus",
               "issue"),
  .con = sqlite_occ
  )
  dbExecute(sqlite_occ, query)
}
```

As issues are semicolon separated (multiple issues could occur), we have to add `'%` before and after the issues for SQLite string matching (`LIKE` operator):

```{r add_%_}
issues_to_discard <- paste0("\'%", issues_to_discard, "%\'")
```

We create the subquery for filtering on issue conditions:

```{r subquery_issues}
issue_condition <- paste("issue NOT LIKE", issues_to_discard, collapse = " AND ")
issue_condition
```

## Create new table with filtered data

New table name: `occ`
 
```{r new_table_name}
table_name_subset <- "occ"
```

We create the new table with selected columns and filtered data on `occurrenceStatus` and `issue`:

```{r make_new_table_subset}
if (!table_name_subset %in% dbListTables(sqlite_occ)) {
  dbCreateTable(conn = sqlite_occ,
               name = table_name_subset,
               fields = preview_df)
  query <- glue_sql(
  "INSERT INTO {small_table} SELECT * FROM {big_table} WHERE 
  LOWER(identificationVerificationStatus) NOT IN ({unverified*}) AND LOWER(occurrenceStatus) NOT IN ({bad_status*}) AND ", issue_condition, 
  small_table = table_name_subset,
  big_table = table_name,
  unverified = identificationVerificationStatus_to_discard,
  bad_status = occurrenceStatus_to_discard,
  .con = sqlite_occ
  )
  dbExecute(sqlite_occ, query)
}
```

## Overview and control filtered data table

## Structure of `occ` table

Check whether the table `occ` has been made:

```{r check_new_table}
table_name_subset %in% dbListTables(sqlite_occ)
```

Columns present:

```{r cols_in_occ}
dbListFields(sqlite_occ, name = table_name_subset)
```

### Check filtered data

We create an index on `occurrenceStatus` to retrieve occurrence status values faster:

```{r create_idx_occStatus}
idx_occStatus <- "idx_occStatus"
# get indexes present on table
query <- glue_sql(
    "PRAGMA index_list({table_name})",
    table_name = table_name_subset,
    .con = sqlite_occ
)
indexes <- dbGetQuery(sqlite_occ, query)
# create index if not present
if (!idx_occStatus %in% indexes$name) {
 query <- glue_sql(
  "CREATE INDEX {idx} ON {table_name} ({cols_idx})",
  idx = idx_occStatus,
  table_name = table_name_subset,
  cols_idx = c("occurrenceStatus"),
  .con = sqlite_occ
  )
 dbExecute(sqlite_occ, query)
}
```

Occurrence status left in the filtered data:

```{r check_occurrenceStatus_values}
query <- glue_sql(
    "SELECT DISTINCT occurrenceStatus FROM {table}",
    table = table_name_subset,
    .con = sqlite_occ
  )
dbGetQuery(sqlite_occ, query)
```

We create an index on `issue` as well:

```{r idx_issue}
idx_issue <- "idx_issue"
if (!idx_issue %in% indexes$name) {
  query <- glue_sql(
    "CREATE INDEX {idx} ON {table_name} ({cols_idx})",
    idx = idx_issue,
    table_name = table_name_subset,
    cols_idx = c("issue"),
    .con = sqlite_occ
  )
  dbExecute(sqlite_occ, query)
}
```

Issues left in the filtered data:

```{r check_issues_values}
query <- glue_sql(
    "SELECT DISTINCT issue FROM {table}",
    table = table_name_subset,
    .con = sqlite_occ
  )
issues_left <- dbGetQuery(sqlite_occ, query)
issues_left
```

Check presence of the unwanted issues in the issues left:

```{r check_filter_on_issue}
any(map_lgl(issues_to_discard, 
            function(issue) {
              any(str_detect(issues_left$issue, issue))
            }))
```

We create an index on `identificationVerificationStatus`:

```{r idx_identificationVerificationStatus}
idx_issue <- "idx_identificationVerificationStatus"
if (!idx_issue %in% indexes$name) {
  query <- glue_sql(
    "CREATE INDEX {idx} ON {table_name} ({cols_idx})",
    idx = idx_issue,
    table_name = table_name_subset,
    cols_idx = c("identificationVerificationStatus"),
    .con = sqlite_occ
  )
  dbExecute(sqlite_occ, query)
}
```

Identification verification status left in the filtered data:

```{r check_identificationVerificationStatus_values}
query <- glue_sql(
    "SELECT DISTINCT identificationVerificationStatus FROM {table}",
    table = table_name_subset,
    .con = sqlite_occ
  )
status_verification_left <- dbGetQuery(sqlite_occ, query)
status_verification_left
```

Number of occurrences left:

```{r n_occs_filtered}
query <- glue_sql(
    "SELECT COUNT() FROM {table}",
    table = table_name_subset,
    .con = sqlite_occ
  )
n_occs <- dbGetQuery(sqlite_occ, query)
n_occs <- n_occs$`COUNT()`
n_occs
```

Overview of all indexes present on `occ`:

```{r index_filtered_table}
query <- glue_sql(
    "PRAGMA index_list({table_name})",
    table_name = table_name_subset,
    .con = sqlite_occ
)
dbGetQuery(sqlite_occ, query)
```

Close connection:

```{r close_connection}
dbDisconnect(sqlite_occ)
```
