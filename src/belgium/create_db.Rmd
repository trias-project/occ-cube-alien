---
title: "Create database and filter data"
author:
- Damiano Oldoni
- Peter Desmet
date: "`r Sys.Date()`"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true
    number_sections: true
---

In this document we transform the GBIF download - a csv file containing the occurrences of (alien) species for Belgium - into a sqlite database, to handle the large volume of data. Then we filter on issues and occurrence status. Note: some of these steps can take long.

# Setup

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

Load libraries:

```{r load_libraries}
library(tidyverse)      # To do datascience
library(here)           # To find files
library(rgbif)          # To use GBIF services
library(glue)           # To write queries
library(RSQLite)        # To interact with SQlite databases
```

# Transform CSV to sqlite file

## Get CSV file from GBIF

Download the occurrences from GBIF, based on the download key returned in `download.Rmd`:

```{r get_occ_file}
key <- "0044021-181108115102211"
download_file <- here("data", "raw", paste0(key, ".zip"))
if (!file.exists(download_file)) {
  occ_download_get(
    key = "0044021-181108115102211", 
    path = here("data", "raw")
  )
}
```

Unzip the occurrence text file from the download file:

```{r unzip_csv}
occ_filename <- paste(key, "occurrence.txt", sep = "_")
# 0044021-181108115102211_occurrence.txt
occ_file <- here("data", "raw", occ_filename)

if (!file.exists(occ_file)) {
  unzip(
    zipfile = download_file,
    files = "occurrence.txt",
    exdir = here("data", "raw")
  )
  file.rename(
    from = here("data", "raw", "occurrence.txt"),
    to = occ_file
  )
}
``` 

Columns names:

```{r get_cols}
cols_occ_file <- read_delim(occ_file, "\t", n_max = 1, quote = "")
cols_occ_file <- names(cols_occ_file)
```

Number of columns present:

```{r n_cols}
length(cols_occ_file)
```

## Create sqlite file

### Database name, path and table name

Define name and path of `.sqlite` file:

```{r sqlite_file}
sqlite_filename <- paste(key, "occurrence.sqlite", sep = "_")
sqlite_file <- here("data", "interim", sqlite_filename)
```

And table name:

```{r define_table_name}
table_name <- "occ_be_all"
```

### Define storage class for each column

The standard storage class is `TEXT`:

```{r def_default_cols_type}
# default type: TEXT
field_types <- rep("TEXT", length(cols_occ_file))
names(field_types) <- cols_occ_file
```

The following columns should be of storage class `INTEGER`:

1. `*Key`, e.g. `taxonKey`, `speciesKey`)
2. `*DayOfYear`: `startDayOfYear` and  `endDayOfYear`  
3. `year`
4. `month`
5. `day`

```{r set_to_integer}
int_fields <- names(field_types)[str_detect(names(field_types), "Key") & 
                                   names(field_types) != "datasetKey"]
int_fields <- c(
  int_fields,
  names(field_types)[str_detect(names(field_types), "DayOfYear")],
  names(field_types)[names(field_types) == "year"],
  names(field_types)[names(field_types) == "month"],
  names(field_types)[names(field_types) == "day"]
)
field_types[which(names(field_types) %in% int_fields)] <- "INTEGER"
```

The following columns should be of storage class `REAL`:

1. `decimal*`: `decimalLatitude` and `decimalLongitude`
2. `coordinate*`: `coordinateUncertaintyInMeters` and `coordinatePrecision`
3. `pointRadiusSpatialFit`

```{r set_to_real}
real_fields <- names(field_types)[str_detect(names(field_types), "decimal")]
real_fields <- c(
  real_fields,
  names(field_types)[str_detect(names(field_types), "coordinate")],
  names(field_types)[names(field_types) == "pointRadiusSpatialFit"]
)
field_types[which(names(field_types) %in% real_fields)] <- "REAL"
```

Open connection to database:

```{r open_connection_to_db}
sqlite_occ <- dbConnect(SQLite(), dbname = sqlite_file)
```

Fill database with occurrences from text file. This step reads the large occurrence file in chunks and transfers them in the sqlite file. This step can take long the first time you run it:

```{r fill_sqlite_file}
if (!table_name %in% dbListTables(sqlite_occ)) {
  dbWriteTable(conn = sqlite_occ,
               name = table_name,
               sep = "\t",
               value = occ_file,
               row.names = FALSE,
               header = TRUE,
               field.types = field_types,
               overwrite = TRUE)
}
```

## Overview

Number of columns present:

```{r check_fields_present}
cols_occ_db <- dbListFields(sqlite_occ, table_name)
length(cols_occ_db)
```

# Filter data

## Define columns to select

We define a subset of columns we are interested to:

```{r columns_to_use}
cols_to_use <- c("gbifID", "scientificName", "kingdom", "phylum", "class", "order",
             "family", "genus", "specificEpithet", "infraspecificEpithet",
             "taxonRank", "taxonomicStatus", "datasetKey", "basisOfRecord",
             "occurrenceStatus", "lastInterpreted", "hasCoordinate",
             "hasGeospatialIssues", "decimalLatitude", "decimalLongitude",
             "coordinateUncertaintyInMeters", "coordinatePrecision",
             "pointRadiusSpatialFit", "verbatimCoordinateSystem", "verbatimSRS",
             "eventDate", "startDayOfYear", "endDayOfYear", "year", "month",
             "day", "verbatimEventDate", "samplingProtocol", "samplingEffort",
             "issue", "taxonKey", "acceptedTaxonKey", "kingdomKey", "phylumKey",
             "classKey", "orderKey", "familyKey", "genusKey", "subgenusKey",
             "speciesKey", "species")
```

Columns in occurrence file not present in the subset:

```{r cols_in_cols_to_use_not_present_in_cols_occ_db}
cols_to_use[which(!cols_to_use %in% cols_occ_db)]
```

will be removed from the selection:

```{r remove_cols_not_in_cols_occ_db}
cols_to_use <- cols_to_use[which(cols_to_use %in% cols_occ_db)]
```

Final number of columns to select:

```{r n_cols_to_use}
length(cols_to_use)
```

Storage class of these columns:

```{r define_field_type_subset}
field_types_subset <- field_types[which(names(field_types) %in% cols_to_use)]
```

## Define filters on occurrences

Occurrences containing the following issues should be filtered out:

```{r issues}
issues_to_discard <- c(
  "ZERO_COORDINATE",
  "COORDINATE_OUT_OF_RANGE", 
  "COORDINATE_INVALID"
)
```

Or occurrences with the following occurrence status:

```{r occurrenceStatus}
occurrenceStatus_to_discard <- c(
  "absent",
  "excluded"
)
```

We create an index based on these two columns if not already present:

```{r create_idx_occStatus_issue}
idx_occStatus_issue <- "idx_occStatus_issue"
# get indexes on table
query <- glue_sql(
    "PRAGMA index_list({table_name})",
    table_name = table_name,
    .con = sqlite_occ
)
indexes_all <- dbGetQuery(sqlite_occ, query)

# create index if not present
if (!idx_occStatus_issue %in% indexes_all$name) {
  query <- glue_sql(
  "CREATE INDEX {`idx`} ON {table_name} ({`cols_idx`*})",
  idx = idx_occStatus_issue,
  table_name = table_name,
  cols_idx = c("occurrenceStatus", "issue"),
  .con = sqlite_occ
  )
  dbExecute(sqlite_occ, query)
}
```

As issues are semicolon separated (multiple issues could occur), we have to add `'%` before and after the issues for SQLite string matching (`LIKE` operator):

```{r add_%_}
issues_to_discard <- paste0("\'%", issues_to_discard, "%\'")
```

We create the subquery for filtering on issue conditions:

```{r}
issue_condition <- paste("issue NOT LIKE", issues_to_discard, collapse = " AND ")
issue_condition
```

## Create new table with filtered data

New table name: `occ_be`
 
```{r new_table_name}
table_name_subset <- "occ_be"
```

We create the new table with selected columns and filtered data on `occurrenceStatus` and `issue`:

```{r make_new_table_subset}
if (!table_name_subset %in% dbListTables(sqlite_occ)) {
  dbCreateTable(conn = sqlite_occ,
               name = table_name_subset,
               fields = field_types_subset)
  query <- glue_sql(
  "INSERT INTO {small_table} SELECT {`some_cols`*} FROM {big_table} WHERE 
  occurrenceStatus NOT IN ({bad_status*}) AND ", issue_condition, 
  small_table = table_name_subset,
  some_cols = names(field_types_subset),
  big_table = table_name,
  bad_status = occurrenceStatus_to_discard,
  .con = sqlite_occ
  )
  dbExecute(sqlite_occ, query)
}
```

## Overview and control filtered data table

## Strructure of `occ_be` table

Check whether the table `occ_be` has been made:

```{r check_new_table}
table_name_subset %in% dbListTables(sqlite_occ)
```

Columns present:

```{r}
dbListFields(sqlite_occ, name = table_name_subset)
```

### Check filtered data

We create an index on occurrences in order to retrieve occurrence status values faster:

```{r create_idx_occStatus}
idx_occStatus <- "idx_occStatus"
# get indexes present on table
query <- glue_sql(
    "PRAGMA index_list({table_name})",
    table_name = table_name_subset,
    .con = sqlite_occ
)
indexes <- dbGetQuery(sqlite_occ, query)
# create index if not present
if (!idx_occStatus %in% indexes$name) {
 query <- glue_sql(
  "CREATE INDEX {idx} ON {table_name} ({cols_idx})",
  idx = idx_occStatus,
  table_name = table_name_subset,
  cols_idx = c("occurrenceStatus"),
  .con = sqlite_occ
  )
 dbExecute(sqlite_occ, query)
}
```

Occurrence status left in the filtered data:

```{r check_occurrenceStatus_values}
query <- glue_sql(
    "SELECT DISTINCT occurrenceStatus FROM {table}",
    table = table_name_subset,
    .con = sqlite_occ
  )
dbGetQuery(sqlite_occ, query)
```

We create an index on `issue` as well:

```{r idx_issue}
idx_issue <- "idx_issue"
if (!idx_issue %in% indexes$name) {
  query <- glue_sql(
    "CREATE INDEX {idx} ON {table_name} ({cols_idx})",
    idx = idx_issue,
    table_name = table_name_subset,
    cols_idx = c("issue"),
    .con = sqlite_occ
  )
  dbExecute(sqlite_occ, query)
}
```

Issues left in the filtered data:

```{r check_issues_values}
query <- glue_sql(
    "SELECT DISTINCT issue FROM {table}",
    table = table_name_subset,
    .con = sqlite_occ
  )
issues_left <- dbGetQuery(sqlite_occ, query)
issues_left
```

Check presence of the unwanted issues in the issues left:

```{r check_filter_on_issue}
any(map_lgl(issues_to_discard, 
            function(issue) {
              any(str_detect(issues_left$issue, issue))
            }))
```

Overview of alll indexes present on `occ_be`:

```{r index_filtered_table}
query <- glue_sql(
    "PRAGMA index_list({table_name})",
    table_name = table_name_subset,
    .con = sqlite_occ
)
dbGetQuery(sqlite_occ, query)
```

Close connection:

```{r close_connection}
dbDisconnect(sqlite_occ)
```
